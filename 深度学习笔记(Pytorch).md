# 深度学习介绍
课程首先讲了一些应用
- 图片分类
- 物体检测和分割
- 样式迁移
- 人脸合成
- 文生图
- 文字生成（LLM）
- 无人驾驶（特斯拉的FSD）
- 案例：广告点击
	1. 触发
	2. 点击率预估（通过模型）
	3. 排序

# 安装环境
### 使用 conda/miniconda 环境
```bash
conda env remove d2l-zh
conda create -n d2l-zh python=3.9 pip -y
conda activate d2l-zh
```

### 安装需要的包
```bash
pip install jupyter d2l torch torchvision -i https://pypi.tuna.tsinghua.edu.cn/simple
```
> **注意：这是一行哦，BTW，代理没关有概率不行，关掉再试哦**

### 下载代码并执行
```bash
wget https://zh-v2.d2l.ai/d2l-zh.zip
unzip d2l-zh.zip
jupyter notebook
```

# 数据操作
### N维数组样例
***N维数组是机器学习和神经网络的主要数据结构***
我把3-d、4-d、5-d数组的内容补充到之前的表格里，让N维数组的示例更完整：

| 维度类型    | 示例图形                             | 数据形式                                                                                                                          | 含义                     |
| ------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ---------------------- |
| 0-d（标量） | ■                                | 1.0                                                                                                                           | 一个类别                   |
| 1-d（向量） | ▬▬▬▬                             | [1.0, 2.7, 3.4]                                                                                                               | 一个特征向量                 |
| 2-d（矩阵） | □□□<br>□□□<br>□□□                | \[\[1.0, 2.7, 3.4]<br>\[5.0, 0.2, 4.6]<br>\[4.3, 8.5, 0.2]]                                                                   | 一个样本—特征矩阵              |
| 3-d     | 🧊（立方体）                          | \[\[\[0.1, 2.7, 3.4]<br>\[5.0, 0.2, 4.6]<br>\[4.3, 8.5, 0.2]]<br>\[\[3.2, 5.7, 3.4]<br>\[5.4, 6.2, 3.2]<br>\[4.1, 3.5, 6.2]]] | RGB图片（宽x高x通道）          |
| 4-d     | 🧊🧊🧊🧊                         | \[\[\[\[.  .   .]]]]                                                                                                          | 一个RGB图片批量（批量大小x宽x高x通道） |
| 5-d     | 🧊🧊🧊🧊<br>🧊🧊🧊🧊<br>🧊🧊🧊🧊 | \[\[\[\[\[.  .  .]]]]]                                                                                                        | 一个视频批量（批量大小x时间x宽x高x通道） |
### 创建数组
创建数组需要指定以下核心要素：
- **形状**：例如 `3×4` 矩阵
- **每个元素的数据类型**：例如32位浮点数
- **每个元素的值**：例如全是0、全是1，或随机数（如正态分布、均匀分布）

### 访问元素
在N维数组（以二维矩阵为例）中，可通过**索引/切片**精准访问元素，以下是常见操作及说明（注：原“一列”的索引写法有误，正确写法应为 `[:,1]`）：
![[Pasted image 20260201191426.png]]

| 访问类型    | 索引写法          | 效果说明                                               | 示例（基于4×4矩阵）                                                                                                                 |
| ------- | ------------- | -------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 单个元素    | `[行索引, 列索引]`  | 定位矩阵中某一个元素（索引从0开始）                                 | `[1,2]` → 选中第2行、第3列的元素 `7`                                                                                                  |
| 整行      | `[行索引, :]`    | 选中某一行的所有元素（`:` 表示“所有”）                             | `[1,:]` → 选中第2行 `[5,6,7,8]`                                                                                                 |
| 整列      | `[:, 列索引]`    | 选中某一列的所有元素（原图中 `[1,:]` 是错误写法，正确为 `[:,1]`）          | `[:,1]` → 选中第2列 `[2,6,10,14]`                                                                                               |
| 子区域（切片） | `[行切片, 列切片]`  | 通过“起始索引:结束索引”（左闭右开）选中连续区域                          | `[1:3, 1:]` → 选中第2-3行、第2列及之后的区域；                                                                                            |
| 间隔切片    | `[::步长,::步长]` | 通过 `::步长` 按指定间隔选元素（`::3` 表示 “从第 0 个开始，每 3 个选 1 个”） | `[::3,::2]` → 行：每 3 行选 1 行（选第 1 行 `[0]` 、第 4 行 `[3]`）；列：每 2 列选 1 列（选第 1 列 `[0]`、第 3 列 `[2]`）→ 最终选中 `[1,3]` 和 `[13,15]` 两个元素 |

### 核心规则
- 索引从 **0** 开始（行/列的第一个元素索引是0）；
- 切片用 `a:b` 表示：包含索引`a`，不包含索引`b`；
- `:` 单独使用表示“选中所有行/列”。
- `::n` 表示 “以 n 为步长，从头到尾间隔选取元素”；

# 数据操作实现
### 入门
- 张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的_向量_（vector）； 具有两个轴的张量对应数学上的_矩阵_（matrix）； 具有两个轴以上的张量没有特殊的数学名称。
- 可以使用 `arange` 创建一个行向量 `x`。它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 _元素_（element）。
```python
import torch

x = torch.arange(12)
print(x)
```

```
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```
- 可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的 _形状_ 。
```python
print(x.shape)
```

```
torch.Size([12])
```
>**如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。**

```python
print(x.numel())
```

```
12
```
- 调用`reshape`函数。 可以改变张量的形状，但不改变其元素值。 
```python
x = x.reshape(3,4)
print(x)
```

```
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
```
> 可以自动计算，使用`x.reshape(-1,4)`或`x.reshape(3,-1)`

- 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。
```python
print(torch.zeros((2,3,4)))
```

```
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```
> 同理可以用`torch.ones((2,3,4))`创建全1的张量

- 也可以使用`randn()`得到随机值
```python
print(torch.randn(3,4))
```

```
tensor([[ 1.2693,  0.5354,  0.2139, -0.1880],
        [-0.3535,  0.8924,  0.0058, -2.2594],
        [ 0.5834, -1.1979, -1.9191,  1.2037]])
```

- 当然也可以用列表为每个元素赋值
```python
print(torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]))
```

```
tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])
```

### 运算符
- 按元素运算
```python
import torch

x = torch.tensor([1.0,2,4,8])
y = torch.tensor([2,2,2,2])

print(f"{x+y},\n{x-y},\n{x*y},\n{x/y},\n{x**y}")
```

```
tensor([ 3.,  4.,  6., 10.]),
tensor([-1.,  0.,  2.,  6.]),
tensor([ 2.,  4.,  8., 16.]),
tensor([0.5000, 1.0000, 2.0000, 4.0000]),
tensor([ 1.,  4., 16., 64.])
```

- 还包括求幂
```python
print(torch.exp(x))
```
>**`exp()`是$exp(x_i​)=e^{x_i}$​

```
tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
```

```python
# 不指定dtype时，默认生成64位整数
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(torch.cat((X, Y), dim=0))
print(torch.cat((X, Y), dim=1))
```
- `dim`参数是连结哪个轴的，0是张量形状的第一个元素是行，以下同理
```
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [ 2.,  1.,  4.,  3.],
        [ 1.,  2.,  3.,  4.],
        [ 4.,  3.,  2.,  1.]])
tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
```
- 通过 _逻辑运算符_ 构建二元张量。
```python
print(X == Y)
```

```
tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])
```
- 求和
```python
print(X.sum())
```

```
tensor(66.)
```

### 广播机制
 在某些情况下，即使形状不同，我们仍然可以通过调用 _广播机制_（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。
> 维数应该相同

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print(a+b)
```

```
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

### 节省内存
- 虽然Python没有C/C++复杂的内存管理，但是节省内存仍然是有必要的
- 可以先看一个例子
```python
before = id(Y)
Y = Y + X
print(id(Y) == before)
```

```
False
```

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
```

```
id(Z): 2377502158336
id(Z): 2377502158336
```

- 当然不复制，直接`X += Y`是可以的
### 转换到其他Python对象
```python
A = X.numpy()
B = torch.tensor(A)
print(type(A), type(B))
```

```python
a = torch.tensor([3.5])
print(a, a.item(), float(a), int(a))
```

